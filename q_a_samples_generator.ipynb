{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# You need to set your OpenAI API key as an environment variable\n",
    "# You can find your API key here: https://beta.openai.com/account/api-keys\n",
    "# If you do not have an API key, you can sign up for free here: https://beta.openai.com/signup\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "DATASET_PATH=\"./dataset\"\n",
    "TOTAL_QUESTIONS_COUNT = 1\n",
    "# For simplicity. Better to calculate tokens count for each text\n",
    "MAX_CONTENT_LENGTH = 4096*2\n",
    "TRANING_DATA_FILEPATH=\"./fine_tuning_dataset.jsonl\"\n",
    "PROMPT_END_SEPARATOR = \"\\n\\n###\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./dataset/Blog - Next.js 13  Next.js.md\n",
      "Processing ./dataset/Getting started with NextUI and Next.js - LogRocket Blog.md\n",
      "Processing ./dataset/Blog - Next.js 13.1  Next.js.md\n",
      "Processing ./dataset/Blog - Next.js 13.2  Next.js.md\n",
      "Processing ./dataset/NextAuth.js for client-side authentication in Next.js  LogRocket Blog.md\n",
      "Processing ./dataset/Next.js 13 Working with the new app directory - LogRocket Blog.md\n",
      "Processing ./dataset/The best new features in Next.js 13  InfoWorld.md\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "questions_answers = []\n",
    "\n",
    "def extract_qa_from_content(content):\n",
    "  blocks = re.split(r'\\n\\n', content)\n",
    "  qa = []\n",
    "\n",
    "  for block in blocks:\n",
    "    question_match = re.search(r'Q:(.+)\\n', block)\n",
    "    answer_match = re.search(r'A:(.+)', block, re.DOTALL)\n",
    "    if question_match and answer_match:\n",
    "      question = question_match.group(1).strip()\n",
    "      answer = answer_match.group(1).strip()\n",
    "      qa.append((question, answer))\n",
    "  \n",
    "  return qa\n",
    "\n",
    "def read_file(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "      content = f.read()\n",
    "  return content\n",
    "\n",
    "def generate_qa(filepath):\n",
    "  article = read_file(filepath)[:MAX_CONTENT_LENGTH]\n",
    "  content = f'''Content for {filepath}:\n",
    "{article}\n",
    "\n",
    "Instructions: Generate question and answer based on Content for {filepath}.\n",
    "Structure it as:\n",
    "Q: <question>\n",
    "A: <answer>\n",
    "'''\n",
    "\n",
    "  questions_answers = []\n",
    "  response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful software developer who specialize in next.js and react.\"},\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "      ],\n",
    "      n=TOTAL_QUESTIONS_COUNT\n",
    "    )\n",
    "  \n",
    "  for choice in response.choices:\n",
    "    qa = extract_qa_from_content(choice.message.content.strip())\n",
    "    questions_answers.extend(qa)\n",
    "  return questions_answers\n",
    "\n",
    "for filename in os.listdir(DATASET_PATH):\n",
    "  filepath = os.path.join(DATASET_PATH, filename)\n",
    "  print(f\"Processing '{filepath}'\")\n",
    "  try:\n",
    "    new_questions_answers = generate_qa(filepath)\n",
    "    questions_answers.extend(new_questions_answers)\n",
    "    print(f\"Generated {len(new_questions_answers)} questions and answers\")\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_qa_to_fine_tuning_dataset(qa):\n",
    "  data = [{\"prompt\": question + PROMPT_END_SEPARATOR, \"completion\": answer} for question, answer in qa]\n",
    "  with open(TRANING_DATA_FILEPATH, 'a') as outfile:\n",
    "    for i, item in enumerate(data):\n",
    "      json.dump(item, outfile)\n",
    "      if i < len(data) - 1:\n",
    "        outfile.write('\\n')\n",
    "\n",
    "write_qa_to_fine_tuning_dataset(questions_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
